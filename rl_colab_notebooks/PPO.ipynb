{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8mkKaEyGGKT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "60215763-20a3-4635-eb22-d31f57505cef"
      },
      "source": [
        "!pip install box2d-py\n",
        "!pip install gym[Box_2D]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |▊                               | 10kB 20.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 6.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 8.9MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 5.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 9.3MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "\u001b[33m  WARNING: gym 0.15.4 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[Box_2D]) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[Box_2D]) (1.17.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym[Box_2D]) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[Box_2D]) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[Box_2D]) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym[Box_2D]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc0h9z2r8X_D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30370adc-faf5-4973-853a-24b818556619"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7IcJ0ezB7E6"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "import gym\n",
        "import sys\n",
        "import tensorflow as tf \n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "\n",
        "actor_model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(128,input_shape=(1,8),activation='relu'),\n",
        "  tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "old_actor_model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(128,input_shape=(1,8),activation='relu', trainable=False),\n",
        "  tf.keras.layers.Dense(4, activation='softmax', trainable=False)\n",
        "])\n",
        "\n",
        "critic_model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(128,input_shape=(1,8),activation='relu'),\n",
        "  tf.keras.layers.Dense(1)\n",
        "])\n",
        "critic_model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate = 0.005))\n",
        "\n",
        "# actor_model.load_weights('/content/gdrive/My Drive/ppo/actor_model10000')\n",
        "# critic_model.load_weights('/content/gdrive/My Drive/ppo/critic_model10000')\n",
        "# old_actor_model.load_weights('/content/gdrive/My Drive/ppo/old_actor_model10000')\n",
        "\n",
        "env = gym.make('LunarLander-v2')\n",
        "env.seed(1)\n",
        "env2 = gym.make('LunarLander-v2')\n",
        "env2.seed(2)\n",
        "# env._max_episode_steps = 1000\n",
        "episodes = 10000\n",
        "score=0\n",
        "episode_n=[]\n",
        "episode_n_test=[]\n",
        "score_train=[]\n",
        "score_test=[]\n",
        "e_clip=0.2\n",
        "ent_coef=0.001\n",
        "\n",
        "Actor_update_steps = 10\n",
        "Critic_update_steps = 10\n",
        "\n",
        "def upd_old_policy():\n",
        "  weights_actor_model = actor_model.get_weights()\n",
        "  old_actor_model.set_weights(weights_actor_model)\n",
        "\n",
        "#@tf.function\n",
        "def losss(states,actions,advantages):\n",
        "   indices=[]\n",
        "\n",
        "   for x in range(len(states)):\n",
        "     indices.append([x,actions[x]])\n",
        "\n",
        "   logits=actor_model(states) \n",
        "\n",
        "   entropy_losses = -tf.reduce_sum(logits *tf.math.log(logits), axis=1)\n",
        "   entropy_losses = tf.reduce_mean(entropy_losses, axis=0)\n",
        "\n",
        "   probs=tf.math.log(tf.gather_nd(logits,tf.convert_to_tensor(indices)))\n",
        "   \n",
        "   logits2=old_actor_model(states)\n",
        "   old_probs=tf.math.log(tf.gather_nd(logits2,tf.convert_to_tensor(indices)))\n",
        "   ratios = tf.exp(probs-old_probs)\n",
        "   clip_probs = tf.clip_by_value(ratios, 1.-e_clip, 1.+e_clip)\n",
        "\n",
        "\n",
        "   loss=-tf.reduce_mean(tf.minimum(tf.multiply(ratios, advantages), tf.multiply(clip_probs, advantages)))\n",
        "   # print(entropy_losses)\n",
        "   loss=loss-ent_coef*entropy_losses\n",
        "   return loss\n",
        "\n",
        "def train(buff):\n",
        "    upd_old_policy()\n",
        "    previous_states= []\n",
        "    real_previous_values=[]\n",
        "    advantages=[]\n",
        "    actions=[]\n",
        "    last_gae = 0.0\n",
        "    GAMMA = 0.99\n",
        "    GAE_LAMBDA = 0.95\n",
        "    for previous_state, action, reward, current_state, done in reversed(buff):\n",
        "        actions.append(action)\n",
        "        previous_states.append(previous_state)\n",
        "        if done:\n",
        "           delta = reward - critic_model(previous_state)\n",
        "           last_gae = delta\n",
        "        else:\n",
        "          delta = reward + GAMMA * critic_model(current_state) - critic_model(previous_state)\n",
        "          last_gae = delta + GAMMA * GAE_LAMBDA * last_gae\n",
        "        advantages.append(last_gae)\n",
        "        real_previous_values.append(last_gae + critic_model(previous_state))\n",
        " \n",
        "    actions=list(reversed(actions))\n",
        "    previous_states=list(reversed(previous_states))\n",
        "    advantages=list(reversed(advantages))\n",
        "    real_previous_values=list(reversed(real_previous_values))\n",
        "\n",
        "\n",
        "    previous_states=np.array(previous_states)\n",
        "    real_previous_values=np.array(real_previous_values)\n",
        "\n",
        "    for _ in range(Critic_update_steps):\n",
        "      critic_model.train_on_batch(previous_states, real_previous_values)    \n",
        "      \n",
        "    advantages=np.vstack(advantages)\n",
        "    \n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "    advantages=tf.squeeze(advantages)\n",
        "    previous_states=np.vstack(previous_states)\n",
        "\n",
        "    for _ in range(Actor_update_steps):\n",
        "      with tf.GradientTape() as tape:\n",
        "        losses=losss(previous_states,actions,advantages)\n",
        "\n",
        "      grads = tape.gradient(losses, actor_model.trainable_variables)\n",
        "      # grads, _ = tf.clip_by_global_norm(grads, 0.5)\n",
        "      optimizer.apply_gradients(zip(grads, actor_model.trainable_variables))\n",
        "    \n",
        "def test():\n",
        "  score=0\n",
        "  for e in range(20):\n",
        "    state = env2.reset()\n",
        "    episode_score = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "       state = state.reshape([1,8])\n",
        "       logits = actor_model(state)\n",
        "       a_dist = logits.numpy()\n",
        "       a = np.random.choice(a_dist[0],p=a_dist[0]) # Choose random action with p = action \n",
        "       a, = np.where(a_dist[0] == a)\n",
        "       a=a[0]\n",
        "       next_state, reward, done, _ = env2.step(a)\n",
        "       episode_score +=reward\n",
        "       state=next_state\n",
        "    score+=episode_score\n",
        "  return (score/20)    \n",
        "\t\n",
        "for e in range(episodes):\n",
        "  state = env.reset()\n",
        "  episode_score = 0\n",
        "  episode_memory=[]\n",
        "  done = False\n",
        "  running_add=0\n",
        "  while not done:\n",
        "    state = state.reshape([1,8])\n",
        "    logits = actor_model(state)\n",
        "    a_dist = logits.numpy()\n",
        "    # env.render()\n",
        "    a = np.random.choice(a_dist[0],p=a_dist[0]) # Choose random action with p = action \n",
        "    a, = np.where(a_dist[0] == a)\n",
        "    a=a[0]\n",
        "    next_state, reward, done, _ = env.step(a)\n",
        "    next_state = next_state.reshape([1,8])\n",
        "    episode_score +=reward\n",
        "\n",
        "    episode_memory.append([state, a, reward, next_state, done])\n",
        "    state=next_state\n",
        "  episode_memory=np.array(episode_memory)\n",
        "  train(episode_memory)\n",
        "  score+=episode_score\n",
        "  \n",
        "  episode_n.append(e+1)\n",
        "  score_train.append(episode_score)\n",
        "  print(\"Episode  {}  Score  {}\".format(e+1, episode_score))\n",
        "  \n",
        "\n",
        "  if (e+1) % 10 == 0:\n",
        "      print(\"10 Episodes  mean train score {}\".format(score/10))\n",
        "      score=0\n",
        "  if(e+1) % 500 == 0:\n",
        "    test_score=test()\n",
        "    episode_n_test.append(e+1)\n",
        "    score_test.append(test_score)\n",
        "\n",
        "    actor_model.save_weights('/content/gdrive/My Drive/ppo/actor_model'+str(e+1))\n",
        "    critic_model.save_weights('/content/gdrive/My Drive/ppo/critic_model'+str(e+1)) \n",
        "    old_actor_model.save_weights('/content/gdrive/My Drive/ppo/old_actor_model'+str(e+1))\n",
        "    \n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(episode_n, score_train)\n",
        "ax.plot(episode_n_test, score_test)\n",
        "ax.set(xlabel='episode n', ylabel='score',title=':(')\n",
        "ax.grid()\n",
        "fig.legend(['Train score', 'Test score'], loc='upper left')\n",
        "fig.savefig(\"ppo_gae.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}